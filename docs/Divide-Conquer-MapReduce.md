# 分治思想——MapReduce

MapReduce 在 Google 大数据处理的三驾马车之一，另外两个时 GFS 和 Bigtable。它在倒排索引、PageRank 计算、网页分析等搜索引擎相关的技术中都有大量的应用。

尽管开发一个 MapReduce 非常高深，困难，但是用到的思想确实我们今天要讲的——分治算法思想。

## 理解分治思想

分治算法的核心就是 “分而治之”，将原始问题切割成 n 个规模较小的，且结构和原问题相似的问题，然后递归的解决的这些子问题，最后合并其结果。

这看起来就跟递归时一样的。关于递归和分治的区别，在排序的时候就讲过：**分治算法是处理问题的一种思想，递归是一种编程技巧。**实际上，分治算法大多数都比较适合用递归来实现。分治算法的递归实现中，我们一般要涉及到以下三种情况：

- 分解：将原问题拆分成多个子问题；
- 解决：递归求解各个子问题，若子问题足够小，则直接求解；
- 合并：合并各个子问题的解成原问题的解。

分治算法要解决的问题，也要符合下面这几个条件：

- 原问题与子问题的结构是相同的；
- 原问题分解成若干个子问题是可以独立求解，子问题之间是互相独立；
- 具有分解终止条件，就是说当问题足够小时，可以直接求解，而不是继续循环分解下去；
- 可以将子问题合并成原问题，并且这个操作复杂度不能太高，否则就起不到减小算法总体复杂度的效果。

## 分治算法应用场景举例

如何用分治算法求得一组数据的有序度和逆序度呢？我们在讲排序算法的时候，提到过有序度和逆序度的概念，也知道在完全有序下的一组数据的有序度时 n(n-1)/2，逆序度是 0。完全逆序的一组数据，则反过来，逆序度是 n(n-1)/2，有序度是 0。

最先想到的是，我们直接从遍历这一组数据，然后拿它与后面的数字一个个做比较。但是这样没比较一个数据就要比较 n 次，有 n 个数组，那么时间复杂度就是 O(n*n)。效率非常低下，那有没有更高效的方法呢？

我们就可以用分治算法来解决这个问题。我们可以将这组数组拆分成前后两个数组 A1 和 A2，分别计算 A1 和 A2 的逆序度对个数 K1 和 K2，然后在计算 A1 和 A2 之间的逆序对个数 K3。那么这个数组的逆序度对个数就是 K1+K2+K3。

我们前面讲过，合并的时候，不能太耗时，否则就起不了降低时间复杂度的作用了。那么我们如何快速的计算 A1 和 A2 之间的逆序度对个数呢？

这里就要借助归并排序了。

归并排序中有一个非常关键的操作，就是将两个有序的小数组，合并成一个有序的数组。实际上，在这个归并的过程中，我们就可以计算出这两个数组的逆序对个数了。每次合并操作，我们就计算逆序对个数，把这些计算出来的逆序对个数求和，就是这个数组的逆序对个数。

![img](https://static001.geekbang.org/resource/image/e8/32/e835cab502bec3ebebab92381c667532.jpg)

尽管我画了一张图来解释，但是我个人觉得，但是还是用代码更好理解：

```c#
private int num = 0; //全局变量或者成员变量
public int Count(int[] a, int n) {
    num = 0;
    MergeSortCounting(a, 0, n - 1);
    return num;
}

private void MergeSortCounting(int[] a, int p, int r) {
    if (p >= r) return;
    int q = (p + r) / 2;
    MergeSortCounting(a, p, q);
    MergeSortCounting(a, q + 1, r);
    Merge(a, p, q, r);
}
private void Merge(int[] a, int p, int q, int r) {
    int i = p, j = q + 1, k = 0;
    int[] tmp = new int[r - p + 1];
    while (i <= q && j <= r) {
        if (a[i] <= a[j]) {
            tmp[k++] = a[i++];
        } else {
            num += (q - i + 1); //统计 p-q 之间，比a[j]大的元素个数
            tmp[k++] = a[j++];
        }
    }
    while (i <= q) { //处理剩下的
        tmp[k++] = a[i++];
    }
    while (j <= r) { //处理剩下的
        tmp[k++] = a[j++];
    }
    for (int i = 0; i <= r - p; i++) { //从tmp拷贝回a
        a[p + i] = tmp[i];
    }
}
```

## 分治思想在海量数据处理的应用

我们一般所讲的数据结构和算法，大部分都是基于内存处理和单机处理的。但是如果处理的问题非常大，超过内存空间大小，没法一次型放到内存中，这个时候，这些数据结构和算法就不管用了。

举个例子，假如有 10GB 的订单文件按照价格排序，由于数据量很大，而一般的机器内存可能只有 2G，4G，所以无法一次性加载内存中进行快速排序，归并排序。

要解决这种海量数据量的问题时，就需要借助分治思想了。我们可以把这些数据按照某个划分方法，划分为几个相对小的数据集，然后把每个小的数据集加载到内存中进行排序，然后在将各个排序后的小数据集在合并。那么这就克服了数据过大，内存有限的限制了，甚至还能利用多线程，分布式来提高效率。

比如我们根据上面的方案，我们可以把这些订单划分为几个金额区间范围。比如 0～1000 元的放到一个小文件，1001～2000 元的为另一个小文件，以此类推。这样每个小文件都能加载到内存中来并排序，最后将每个排序后的小文件合并，最终就是 10G 文件排序后的数据了。

如果订单数据存储在类似 GFS 这样的分布式系统中，当 10GB 的订单被划分为多个区间的小文件时，每个文件并行加载到多个处理器处理，最后将结果合并到一起，这样并行的速度也加快了很多。不过，这里有一个注意的地方：就是数据的存储与计算所在的机器是同一个或者在网络中靠的很近（比如一个局域网内，数据存取速度很快），否则就会因为数据访问的速度，导致整个处理过程不但不会加快，甚至还会减慢。

## 总结

分治算法可以用四个字概括就是 “分而治之”，**将问题划分成 n 个规模较小的而结构与原问题相似的子问题，递归地解决这些子问题，然后再将其合并，就得到原问题的解**。